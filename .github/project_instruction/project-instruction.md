Okay, here is the translated and restructured project overview based on the document you provided.

### Project Overview

This project aims to build a recruitment platform using React.js for the frontend, Nest.js for the backend, PostgreSQL for the database, and Python for web scraping and AI analysis services. The platform includes client and portal frontends, offering features like job searching, resume uploading, and AI matching.

### Technology Stack

* **Frontend:** React.js (Single Page Application - SPA) 
* **Backend:** Nest.js 
* **Database:** PostgreSQL 
* **Crawler/AI:** Python 

### Project Structure

1.  **React.js Frontend (SPA)** 
    * `client-frontend`: User interface for job seekers and visitors (React).
    * `portal-frontend`: User interface for HR and recruitment managers (React).
    * `admin-frontend`: User interface for administrators (React).
2.  **Nest.js Backend** 
    * `client-backend`: API for the client frontend (Nest.js).
    * `portal-backend`: API for the portal and admin frontends (Nest.js).
    * These backends share the same database and potentially the same API endpoints, controlled via routing and role-based access control.
3.  **Python Services** 
    * `crawler`: Scrapy web crawler (Python).
    * `ai-analysis`: Gemini API for matching and Natural Language Processing (NLP) preprocessing (Python).
4.  **Database** 
    * PostgreSQL.

### User Roles and Permissions

* **`client-frontend`**: Primarily for job seekers and visitors, requiring features like job search, resume upload, and job matching.
* **`portal-frontend`**: Primarily for recruitment managers and HR.
* **`admin-frontend`**: Primarily for administrators.

### Job Data Source and Crawler

* The `crawler` service will initially scrape JobsDB, with plans to add other recruitment sites later.
* The required scraping frequency is once per day.
* Data scraped from JobsDB needs sophisticated semantic deduplication to identify similar jobs.
* Data standardization is needed for fields like location names, salary formats, and skill tags.
* There's no initial preference for the PostgreSQL table structure for job data, but there's a desire to differentiate between raw and cleaned data.

### Resume Processing and AI Analysis

* Resume uploads support only PDF and DOCX formats.
* The `ai-analysis` service needs to extract years of work experience, education level and major, industry of previous employers, and job-related skills from resumes.
* AI matching logic includes recommending jobs based on resumes and recommending candidates based on job descriptions.
* The Gemini API will be used for semantic understanding, skill standardization, summary generation, and match scoring.
* Extracted information should be detailed, e.g., "Experience 5-7 years," with skills simply listed.
* The acceptable matching precision is roughly 50% high relevance within the Top 10 recommendations.
* Match results need to consider multiple factors (skill match, experience match, location fit, etc.) and provide a composite score.
* The primary focus for Gemini API usage is understanding the deep semantics of job descriptions and resumes to improve matching accuracy, followed by standardizing skill tags and automatically generating job/candidate summaries.
* Match scoring will consider skills, experience, location, etc., to generate a composite score, which can be adjusted later as needed.
* **Gemini API Usage Considerations:** Due to budget and potential API call cost/frequency limits (personal project), strategies include processing only new/updated data, prioritizing lower-cost models (like Gemini 1.5 Flash), implementing caching, logging API requests/responses/costs, and handling specific Gemini API errors. The auto-summary feature will be default-on but controllable via a switch.
* **AI-Driven Data Standardization:** The system (potentially using AI) should automatically infer or generate standard formats for location, salary, and skills.
    * Location: Standardize to "Country-City" level.
    * Salary: Standardize to annual range with currency specified.
    * Skills: Map to a predefined skill library/taxonomy.
* **Standardization Workflow & Review:**
    * The "standard skill library" will initially be generated by analyzing existing job/resume data, requiring manual review and editing. Updates will follow the same process.
    * AI-inferred standardizations for location and salary require a manual review step. Locations should be "Country-City"; unclear data should be marked as null for manual review by an administrator.
    * Administrators perform these reviews in `admin-frontend`. A "reviewed" flag (e.g., `is_reviewed=true` in the database) marks completion.
* **Resume Parsing:** When extracting skills, "simple listing" means extracting potential skill terms first, followed by basic filtering or categorization.
* **Matching Precision & Feedback:** The target is 50% high relevance in Top 10 matches. There are plans to allow users (job seekers or HR) to provide feedback on match results for future model/weight optimization.

### Core Functionality Details

* **Job Search:** Must support filtering and sorting by keywords, location, salary range, company name, company industry, posting date, job type, and skills.
* **Administrator (`admin-frontend`)**: Needs user management, job posting/editing/delisting, candidate management, viewing match results, data statistics (e.g., job posting counts, application counts, match success rates), and management of crawler/AI processes. Requires an interface for reviewing standardized data (location, salary, pending skills, potentially inaccurate company info) with approval/rejection/editing capabilities, search, and filtering for each review type.
* **Job Seeker Personal Center (`client-frontend`)**: Manage resumes, application history, saved jobs, edit personal profile, set job preferences.
* **HR/Recruitment Manager (`portal-frontend`)**: Basic job and candidate management. Can proactively search the entire platform's resume database and view all job seeker resume information. No complex workflow support or permission controls foreseen initially.

### Technical Implementation Details

* `client-backend`, `portal-backend`, and `admin-backend` are planned as different modules within the same Nest.js application, differentiated by routing and simple Role-Based Access Control (RBAC) for Job Seeker, HR, and Admin roles.
* Communication between the three frontends and the Nest.js backend will use standard RESTful APIs.
* API versioning strategy is not considered for now.
* Communication between the backend and Python services will initially use RESTful APIs. gRPC or a lightweight message queue might be considered later for better efficiency, reliability, and decoupling.
* Authentication will initially only require username/password; no other mechanisms needed for now.
* User registration requires only password and role selection initially.
* **Daily Batch Processing:** This integrates data cleaning, standardization attempts (location/salary/skills), company info processing, populating `job_skills`, and AI match pre-calculation. This large task might be broken down into smaller serial/parallel sub-tasks (e.g., Clean -> Standardize -> Skill Map -> Company Info -> Match Pre-calculation) for robustness and efficiency.
* **Asynchronous Tasks & Communication:** For background tasks (daily batch, async resume parsing) and communication between services (Nest.js, Crawler, AI Analysis), a message queue (like Redis Queue, RabbitMQ, BullMQ) will be used for reliable asynchronous communication, with Nest.js potentially acting as the orchestrator.

### Non-Functional Requirements

* **Performance/Scalability:** Initially used by one person. Expected future load is under 100 users and potentially 100,000+ jobs.
* **Deployment:** Will use Docker and AWS after development.
* **Initial Data Import:** Need to import existing data (tens of thousands of jobs/resumes currently in consistent CSV/Excel format). The backend needs a CSV/Excel import function to load this data into a `jobs_raw` table, letting it go through the system's cleaning/standardization process. Import should consider posting dates to handle potential duplicates.

### Minimum Viable Product (MVP)

The MVP includes job listing, job search, and the job crawler.
1.  **Phase 1:** User system (register/login), job listing, basic job search (using SQL LIKE or PostgreSQL full-text search on indexed fields in `jobs_cleaned`). Initial sorting by post date or simple match score (displaying the score).
2.  **Phase 2:** Basic backend admin interface and job crawler functionality.
    * **User Management:** View user list (username/email, role), manually modify roles.
    * **Crawler Monitoring:** Simple UI showing recent run time, status (success/fail/running), basic stats (raw jobs count), and error log access on failure. A button to manually trigger the crawler task before full automation.
    * **Data Overview:** Display key metrics (total `jobs_raw`, `jobs_cleaned` counts, pending review counts).
    * **Manual Crawler Trigger:** Button for debugging/emergency use.
3.  **Phase 3:** Resume handling (upload/parsing). Parsing is an asynchronous background task via the `ai-analysis` service. Parsed data stored in a JSON field (`parsed_data`) with a defined structure (profile, summary, work experience, education, projects, certifications, skills, languages).
4.  **Phase 4:** Core AI functions (matching/standardization). New potential skills found trigger a standardization workflow: temporarily store them, mark status as 'pending' in the `skills` table for admin review before adding to the standard library. Rejected skills might be re-extracted later. AI matching (resume<->job) scores are pre-calculated daily in the background and stored. A dedicated table like `match_scores` (with `resume_id`, `job_id`, `score`, optional `score_details`, `calculated_at`) is suggested for storing these pre-calculated scores.

### Data Handling & Recommendations

* **Job Deduplication:** Start with a simpler text-based similarity approach (e.g., TF-IDF + Cosine Similarity on key job description fields) before moving to complex semantic models. Use a similarity threshold (e.g., 0.85-0.9) to identify duplicates. Python's scikit-learn can be used. The threshold can be adjusted based on results.
* **Skill Mapping:**
    1.  **Build Standard Library:** Use Gemini API on job descriptions to extract and standardize skills (e.g., map "Java Development", "Java EE" to "Java"), stored in a `skills` table.
    2.  **Extract Resume Skills:** Simple list extraction from resumes.
    3.  **Map:** Match extracted resume skills to the standard library via direct matching or fuzzy/semantic matching (using Gemini API similarity score above a threshold, e.g., "JS" -> "JavaScript").
    4.  **Store:** Use many-to-many association tables (`job_skills`, `resume_skills`).
    5.  **Refinement:** If initial extraction is poor, add smarter filtering or manual review.
* **Match Scoring Weights (Initial Recommendation):** Start with a baseline and iterate. Suggested initial weights: Skill Match (50-60%), Experience Match (20-30%), Location Fit (10-20%), Other factors (remaining %). Calculate normalized scores per factor, then use weighted sum. Adjust based on real-world feedback (via conversation initially) after full development.
* **Error Handling & Logging:**
    * **Error Handling:** Use Nest.js exception filters (unified JSON error format), React Error Boundaries, and try...catch blocks in Python services (especially for API/DB calls, gRPC communication).
    * **Logging:** Use standard levels (DEBUG, INFO, WARN, ERROR, FATAL), structured JSON logging (with timestamp, level, service name, Trace ID, message, stack trace), log key events/API calls/errors. Use libraries like Nest.js Logger/winston/pino, Python's `logging`, potentially frontend logging.
    * **Trace ID:** Generate and pass a unique Trace ID across services for request tracing.
    * **Log Storage:** Initially save logs as CSV, potentially moving to CloudWatch later.
* **Database Structure (Initial Recommendation):**
    * `jobs_raw`: id, source, raw_data (JSON/TEXT), crawled_at, processed_at.
    * `jobs_cleaned`: id, job_title, company_name (potentially FK to `companies`), location_standardized, salary_standardized, description, requirements, posted_at, source_url, created_at, updated_at, employment_type, experience_level_required, education_requirement, work_location_type.
    * `skills`: id, skill_name, category, sub_category, description, status ('pending'/'approved'/'rejected').
    * `job_skills`: job_id (FK), skill_id (FK).
    * `users`: id, email/username, password_hash, role ('job_seeker', 'hr', 'admin'), created_at.
    * `resumes`: id, user_id (FK), file_path, extracted_text (TEXT), parsed_data (JSON), uploaded_at.
    * `resume_skills`: resume_id (FK), skill_id (FK).
    * `applications`: id, user_id, job_id, resume_id, applied_at, status.
    * `job_seeker_profiles`: user_id (PK/FK), preferences (JSON), profile_summary (TEXT).
    * `companies`: id, name, industry, size, description, etc. (Linked from `jobs_cleaned`). Information sourced automatically from job descriptions/web scraping, with manual admin editing allowed.
    * `match_scores`: id, resume_id (FK), job_id (FK), score, score_details (JSON), calculated_at.
* **Data Flow:**
    * `jobs_raw` to `jobs_cleaned` conversion involves HTML cleaning, structured info extraction, and standardization based on DB-stored enums/levels for fields like employment type, experience level, education, work location type.
    * `job_skills` and `resume_skills` are populated during the daily batch cleaning (for jobs) and asynchronous resume parsing (for resumes) after skills are identified and standardized.
    * The entire processing flow (cleaning, standardization, mapping, matching) runs as a batch job after the daily crawl. Manual review is a separate, subsequent asynchronous task.